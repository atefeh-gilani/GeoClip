# -*- coding: utf-8 -*-
"""GeoClip.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZspaxmxSSMB-TpDRJKgsRDjyCsQTLJKB
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset, Dataset, TensorDataset
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
import random
from scipy.linalg import sqrtm


"""
m is the mean (offset), M is the linear transformation matrix, and C is the clipping threshold.
sigma denotes the standard deviation of the added Gaussian noise.

The function Geo_module maps the gradient into a transformed space, applies standard DP-SGD
(i.e., gradient clipping followed by additive Gaussian noise), and then maps the
result back to the original parameter space.
"""

def Geo_module(g, m, M, M_left, sigma, C=1):
    w = M @ (g - m)
    w_norm = torch.linalg.vector_norm(w, ord=2)
    w_hat = w / torch.max(torch.tensor(1.0), (w_norm / C))

    noise = torch.randn_like(w_hat) * sigma
    w_tilde = w_hat + noise

    g_tilde = M_left @ w_tilde + m

    return g_tilde

# GeoClip algorithm
def geoclip(model, criterion, train_loader, test_loader, learning_rate,
            batch_size, sigma, num_epochs, beta1=0.99, beta2=0.999,
            h1=1e-15, h2=10, C=1):

    d = sum(p.numel() for p in model.parameters())

    # Initialize mean and covariance parameters
    m = torch.zeros(d)
    cov = torch.eye(d)
    M = torch.eye(d)
    M_left = torch.eye(d)

    theta = torch.cat([p.view(-1) for p in model.parameters()])
    optimizer = optim.SGD(model.parameters(), lr=learning_rate)


    train_losses = []
    mse_list = []

    def set_param_vector(vec):
        pointer = 0
        for p in model.parameters():
            numel = p.numel()
            p.data = vec[pointer:pointer+numel].view(p.size())
            pointer += numel

    # Training loop
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for iteration, (inputs, labels) in enumerate(train_loader):

            theta = torch.cat([p.view(-1) for p in model.parameters()])
            batch_grads = []

            for i in range(inputs.size(0)):
                x_i = inputs[i].unsqueeze(0)
                y_i = labels[i].unsqueeze(0)

                # Forward pass
                outputs = model(x_i)
                loss = criterion(outputs, y_i)

                # Compute gradients
                model.zero_grad()
                loss.backward()

                g_i = torch.cat([p.grad.view(-1) for p in model.parameters()])

                # Apply Geo Module
                g_i_tilde = Geo_module(g_i, m, M, M_left, sigma, C)

                batch_grads.append(g_i_tilde)

            # Compute average noisy gradient
            g_tilde = torch.stack(batch_grads).mean(dim=0)

            # Update model parameters
            theta = theta - learning_rate * g_tilde
            set_param_vector(theta)

            # Update mean
            m = beta1 * m + (1 - beta1) * g_tilde

            # Compute eighvals and eighvecs
            g_mat = torch.stack(batch_grads) - g_tilde
            g_cov = torch.cov(g_mat.T)
            cov = beta2 * cov + (1 - beta2) * g_cov
            eighvals, eighvecs = torch.linalg.eigh(cov)
            S = eighvals.flip(0)
            V = eighvecs.flip(1)
            S= torch.clamp(S,min=h1, max=h2)

            trace = torch.sum(torch.sqrt(S))
            M_coeff = (1 / trace) ** (1/2)
            M_mat = torch.diag(S ** (-1/4)) @ V.T
            M = M_coeff * M_mat

            M_inv_coeff = (1 / trace) ** (-1/2)
            M_inv_mat = V @ torch.diag(S ** (1/4))
            M_left = M_inv_coeff * M_inv_mat

            # Record loss
            running_loss += loss.item()

        # Compute average loss for the epoch
        epoch_loss = running_loss / len(train_loader)
        train_losses.append(epoch_loss)


        # Evaluate on test set
        model.eval()
        preds_list = []
        targets_list = []
        with torch.no_grad():
            for inputs, targets in test_loader:
                outputs = model(inputs)
                preds_list.append(outputs)
                targets_list.append(targets)

        # Concatenate all predictions and targets
        preds = torch.cat(preds_list, dim=0)
        targets = torch.cat(targets_list, dim=0)

        # Compute error metrics
        mse = torch.mean((preds - targets) ** 2).item()
        mse_list.append(mse)

        print(f"Epoch {epoch+1}/{num_epochs}, Test MSE: {mse:.4f}")

    return train_losses, mse_list