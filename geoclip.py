# -*- coding: utf-8 -*-
"""sept11-linreg-synth-for-git.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tc-dT_rqbvakHqdu1NcEuuAOb_H-wttV
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset, Dataset, TensorDataset
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
import random
from scipy.linalg import sqrtm

def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Set seed
set_seed(42)

# GeoClip privacy engine
def privacy_engine_mini_batch_transformed_basis(g_batch, m, M, sigma, random_seed=42):
    def check_nan(var, name):
        if torch.isnan(var).any():
            print(f"NaN detected in variable: {name}")
            nan_idx = torch.isnan(var).nonzero()
            print(f"Indices of NaNs: {nan_idx}")
            raise ValueError(f"NaN detected in {name}")
    if random_seed is not None:
        torch.manual_seed(random_seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(random_seed)

    g_centered = g_batch - m
    check_nan(g_centered, "g_centered")
    w = g_centered @ M.T
    check_nan(w, "w")
    w_norms = torch.linalg.vector_norm(w, ord=2, dim=1, keepdim=True)
    check_nan(w_norms, "w_norms")
    scale = torch.clamp(w_norms, min=1.0)
    check_nan(scale, "scale")
    w_hat = w / scale
    w_hat_ave = w_hat.mean(dim=0)

    # Correct noise variance scaling
    noise = torch.randn_like(w_hat_ave) * (sigma / g_batch.size(0))
    w_tilde_ave = w_hat_ave + noise

    return w_tilde_ave

# Utility functions
def eval_regression(model, test_loader):
    # Evaluate on test set
    model.eval()
    preds_list = []
    targets_list = []
    with torch.no_grad():
        for inputs, targets in test_loader:
            outputs = model(inputs)
            preds_list.append(outputs)
            targets_list.append(targets)

    # Concatenate all predictions and targets
    preds = torch.cat(preds_list, dim=0)
    targets = torch.cat(targets_list, dim=0)

    mse = torch.mean((preds - targets) ** 2).item()
    return mse

def eval_classification(model, test_loader):
    # Evaluate on test set
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    test_accuracy = 100 * correct / total
    return test_accuracy

import matplotlib.pyplot as plt

def plot_training_results(train_losses, train_losses_iter, test_metrics, task_type="regression"):
    if task_type == "regression":
        metric_name = "MSE"
        title = "Test MSE"
        ylabel = "Loss"
    elif task_type == "classification":
        metric_name = "Accuracy"
        title = "Test Accuracy"
        ylabel = "Accuracy (%)"
    else:
        raise ValueError(f"Unknown task_type: {task_type}")

    plt.figure(figsize=(15, 5))

    # Epoch-level training loss
    plt.subplot(1, 3, 1)
    plt.plot(train_losses)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')

    # Iteration-level training loss
    plt.subplot(1, 3, 2)
    plt.plot(train_losses_iter)
    plt.title('Training Loss')
    plt.xlabel('Iteration')
    plt.ylabel('Loss')

    # Test metric
    plt.subplot(1, 3, 3)
    plt.plot(test_metrics)
    plt.title(title)
    plt.xlabel('Epoch')
    plt.ylabel(ylabel)

    plt.tight_layout()
    plt.savefig('geoclip_training_results.png')
    plt.show()

# GeoClip algorithm
def geoclip(model, criterion, train_loader, test_loader, task_type,
            learning_rate, batch_size, sigma, num_epochs,
            h1=1e-8, h2=1e8, beta1=0.9, beta2=0.999,
            apply_geoclip=True):

    d = sum(p.numel() for p in model.parameters())

    # Initialize mean and covariance parameters
    initial_beta2 = beta2
    m = torch.zeros(d)
    cov = torch.eye(d)
    M = torch.eye(d)
    M_left = torch.eye(d)

    # Initialize parameter vector (default random initialization)
    theta = torch.cat([p.view(-1) for p in model.parameters()])

    # Initialize optimizer
    optimizer = optim.SGD(model.parameters(), lr=learning_rate)

    train_losses = []
    train_losses_iter = []
    metric_list = []

    # Update model parameters
    def set_param_vector(vec):
        pointer = 0
        for p in model.parameters():
            numel = p.numel()
            p.data = vec[pointer:pointer+numel].view(p.size())
            pointer += numel

    started_m_update = False
    # Check privacy flag
    if apply_geoclip:
        print("Applying geometry-aware clipping.")

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for iteration, (inputs, labels) in enumerate(train_loader):
            # Get current parameter vector
            t = epoch * len(train_loader) + iteration + 1
            beta2 = initial_beta2 / (1 + 0.01 * t)
            theta = torch.cat([p.view(-1) for p in model.parameters()])

            # Compute gradients for each example in batch
            per_sample_grads = []

            for i in range(inputs.size(0)):
                x_i = inputs[i].unsqueeze(0)
                y_i = labels[i].unsqueeze(0)

                # Forward pass
                outputs = model(x_i)
                loss = criterion(outputs, y_i)

                # Compute gradients
                model.zero_grad()
                loss.backward()

                # Extract gradients
                g_i = torch.cat([p.grad.view(-1) for p in model.parameters()])
                per_sample_grads.append(g_i)

            # Stack gradients into shape [batch_size, d] and compute average
            g_batch = torch.stack(per_sample_grads)
            g_mean=g_batch.mean(dim=0)

            w_tilde_ave=privacy_engine_mini_batch_transformed_basis(g_batch, m, M, sigma)
            #print('w_tilde_ave:',w_tilde_ave)
            g_tilde_ave = w_tilde_ave @ M_left.T + m
            #print('g_tilde_ave:',g_tilde_ave)

            # Update model parameters
            theta = theta - learning_rate * g_tilde_ave
            set_param_vector(theta)

            # Check if it's time to start updating
            if apply_geoclip and not started_m_update:
                started_m_update = True

            # Only update M and M_left if flag is set
            if started_m_update:
                # Update mean and covariance
                m = beta1 * m + (1 - beta1) * g_tilde_ave
                #print('m:',m)
                g_tilde_cen=g_tilde_ave-m
                #print('g_tilde_cen:',g_tilde_cen)
                cov_g_tilde = g_batch.size(0)*(torch.ger(g_tilde_cen, g_tilde_cen))
                #print('cov_g_tilde:',cov_g_tilde)
                cov = beta2 * cov + (1 - beta2) * cov_g_tilde

                # Eigen-decomposition
                eighvals, eighvecs = torch.linalg.eigh(cov)
                S = eighvals.flip(0)
                V = eighvecs.flip(1)
                S= torch.clamp(S,min=h1, max=h2)

                # Update_M
                trace = torch.sum(torch.sqrt(S))
                M_coeff = (1 / trace) ** (1/2)
                M_mat = torch.diag(S ** (-1/4)) @ V.T
                M = M_coeff * M_mat

                # Update M_left
                M_inv_coeff = (1 / trace) ** (-1/2)
                M_inv_mat = V @ torch.diag(S ** (1/4))
                M_left = M_inv_coeff * M_inv_mat

            # Record loss
            running_loss += loss.item()
            train_losses_iter.append(loss.item())

        # Compute average loss for the epoch
        epoch_loss = running_loss / len(train_loader)
        train_losses.append(epoch_loss)

        if task_type == "regression":
            mse = eval_regression(model, test_loader)
            metric_list.append(mse)
            print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Test MSE: {mse:.4f}")

        elif task_type == "classification":
            acc = eval_classification(model, test_loader)
            metric_list.append(acc)
            print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Test Accuracy: {acc:.2f}%")

        else:
            raise ValueError(f"Unknown task_type: {task_type}")

    # Plot results
    plot_training_results(train_losses, train_losses_iter, metric_list, task_type)

    return metric_list