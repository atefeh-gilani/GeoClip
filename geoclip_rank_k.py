# -*- coding: utf-8 -*-
"""oct7-online-pca-git.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r75yQk6MdCrPk-ZkG1R4EzFWeog65A7Y
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset, Dataset, TensorDataset
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
import random
from scipy.linalg import sqrtm
from torch.nn import BCEWithLogitsLoss

def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Set seed
set_seed(42)

# Rank-k+1 PCA update
def rank_k_plus_1_update(V, Lambda, g_centered, beta, k):
    d, current_k = V.shape
    assert current_k == k, f"Expected V to have k={k}, got {current_k}"
    x = g_centered.view(-1, 1)
    U_aug = torch.cat([V, x], dim=1)
    M_aug = torch.diag(torch.cat([beta * Lambda, torch.tensor([1 - beta], device=V.device)]))
    Z = U_aug @ torch.linalg.cholesky(M_aug + 1e-6 * torch.eye(k + 1, device=V.device))
    # Z = U_aug @ torch.linalg.cholesky(M_aug)
    U_new, S_new, _ = torch.linalg.svd(Z, full_matrices=False)
    return U_new[:, :k], S_new[:k]**2

# GeoClip privacy engine
def privacy_engine_mini_batch_transformed_basis(g_batch, m, M, sigma, random_seed=42):
    if random_seed is not None:
        torch.manual_seed(random_seed)
    g_centered = g_batch - m
    w = g_centered @ M.T
    w_norms = torch.linalg.vector_norm(w, ord=2, dim=1, keepdim=True)
    w_hat = w / torch.clamp(w_norms, min=1.0)
    w_hat_ave = w_hat.mean(dim=0)
    noise = torch.randn_like(w_hat_ave) * (sigma / g_batch.size(0))
    return w_hat_ave + noise

# GeoClip with rank-k update
def geoclip_with_rank_k_update(model, criterion, train_loader, test_loader,
                              learning_rate, batch_size, sigma, num_epochs, k,
                              h1=1e-8, h2=10, beta1=0.9, beta2=0.999,
                              eta=0.01, alpha=0.98, gamma=1):

    d = sum(p.numel() for p in model.parameters())

    # Initialize parameters
    m = torch.zeros(d)
    V = torch.eye(d, k) + 1e-4 * torch.randn(d, k)
    V, _ = torch.linalg.qr(V)
    eigvals = torch.ones(k)
    theta = torch.cat([p.view(-1) for p in model.parameters()])
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)

    train_losses, train_losses_iter, test_losses, test_accuracies = [], [], [], []

    started_m_update = False

    # Update model parameters
    def set_param_vector(vec):
        pointer = 0
        for p in model.parameters():
            numel = p.numel()
            p.data = vec[pointer:pointer + numel].view(p.size())
            pointer += numel

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for iteration, (inputs, labels) in enumerate(train_loader):
            t = epoch * len(train_loader) + iteration + 1

            # Compute gradient for each sample in batch
            per_sample_grads = []

            for i in range(inputs.size(0)):
                x_i = inputs[i].unsqueeze(0)
                y_i = labels[i].unsqueeze(0)
                outputs = model(x_i)
                loss = criterion(outputs, y_i)
                model.zero_grad()
                loss.backward()
                g_i = torch.cat([p.grad.view(-1) for p in model.parameters()])
                per_sample_grads.append(g_i)

            # Transform basis and add noise
            g_batch = torch.stack(per_sample_grads)
            g_mean = g_batch.mean(dim=0)

            if not started_m_update:
                g_tilde = g_mean
            else:
                w_tilde = privacy_engine_mini_batch_transformed_basis(g_batch, m, M, sigma)
                g_tilde = M_left @ w_tilde + m

            # Update model parameters
            theta = theta - learning_rate * g_tilde
            set_param_vector(theta)

            # Update mean
            m = beta1 * m + (1 - beta1) * g_tilde
            g_centered = g_tilde - m

            # Rank-k PCA step
            V, eigvals = rank_k_plus_1_update(V, eigvals, g_centered, alpha, k)
            eigvals_clip = torch.clamp(eigvals, min=h1, max=h2)
            trace = torch.sum(torch.sqrt(eigvals_clip))

            # Update M and M_left
            M_coeff = (gamma / trace) ** 0.5
            M = M_coeff * (torch.diag(eigvals_clip ** (-1 / 4)) @ V.T)
            M_inv_coeff = (gamma / trace) ** -0.5
            M_left = M_inv_coeff * (V @ torch.diag(eigvals_clip ** (1 / 4)))

            started_m_update = t > 1

            # Record loss
            running_loss += loss.item()
            train_losses_iter.append(loss.item())

            # Evaluate on test set
            model.eval()
            all_logits, all_labels = [], []
            with torch.no_grad():
                for val_inputs, val_labels in test_loader:
                    logits = model(val_inputs)
                    all_logits.append(logits)
                    all_labels.append(val_labels)

            all_logits = torch.cat(all_logits)
            all_labels = torch.cat(all_labels)
            preds = torch.argmax(all_logits, dim=1)
            acc = (preds == all_labels).float().mean().item()
            test_loss = criterion(all_logits, all_labels).item()

            test_losses.append(test_loss)
            test_accuracies.append(acc)

            print(f"Epoch {epoch+1}, Iter {t} | Train Loss: {loss.item():.4f} | Test Loss: {test_loss:.4f} | Test Acc: {acc:.4f}")

        # Compute average loss for the epoch
        train_losses.append(running_loss / len(train_loader))
        scheduler.step()

    return train_losses, train_losses_iter, test_losses, test_accuracies